%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Stylish Article
% LaTeX Template
% Version 2.1 (1/10/15)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[fleqn,10pt]{SelfArx} % Document font size and equations flushed left

\usepackage[english]{babel} % Specify a different language here - english by default

\usepackage{lipsum} % Required to insert dummy text. To be removed otherwise

\captionsetup[figure]{justification=justified, singlelinecheck=off} 
\captionsetup[table]{justification=justified, singlelinecheck=off} 

%----------------------------------------------------------------------------------------
%	COLUMNS
%----------------------------------------------------------------------------------------

\setlength{\columnsep}{0.55cm} % Distance between the two columns of text
\setlength{\fboxrule}{0.75pt} % Width of the border around the abstract
\linespread{1.8}

%----------------------------------------------------------------------------------------
%	COLORS
%----------------------------------------------------------------------------------------

\definecolor{color1}{RGB}{0,0,90} % Color of the article title and sections
\definecolor{color2}{RGB}{10,20,20} % Color of the boxes behind the abstract and headings
\definecolor{xsubj}{RGB}{243,194,68} 
\definecolor{xsess}{RGB}{53,99,161} 
\definecolor{xsamp}{RGB}{18,165,121} 

%----------------------------------------------------------------------------------------
%	HYPERLINKS
%----------------------------------------------------------------------------------------

\usepackage{hyperref} % Required for hyperlinks
\hypersetup{hidelinks,colorlinks,breaklinks=true,urlcolor=color2,citecolor=color1,linkcolor=color1,bookmarksopen=false,pdftitle={Title},pdfauthor={Author}}
%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% \JournalInfo{Journal, Vol. XXI, No. 1, 1-5, 2013} % Journal information
\JournalInfo{$ $ } % Journal information
\Archive{Pre-print} % Additional notes (e.g. copyright, DOI, review/research article)

\PaperTitle{Numerical Uncertainty in Analytical Pipelines Lead to Impactful Variability in Brain Networks} % Article title

\Authors{Gregory Kiar\textsuperscript{1}, Yohan Chatelain\textsuperscript{2}, Pablo de Oliveira Castro\textsuperscript{3},
Eric Petit\textsuperscript{4}, Ariel Rokem\textsuperscript{5}, Gaël Varoquaux\textsuperscript{6},
Bratislav Misic\textsuperscript{1}, Alan C. Evans\textsuperscript{1$\dagger$}, Tristan Glatard\textsuperscript{2$\dagger$}} % Authors
\affiliation{\textsuperscript{1}\textit{Montréal Neurological Institute, McGill University, Montréal, QC, Canada}}
\affiliation{\textsuperscript{2}\textit{Department of Computer Science and Software Engineering, Concordia University, Montréal, QC, Canada}}
\affiliation{\textsuperscript{3}\textit{Department of Computer Science, Université of Versailles, Versailles, France}}
\affiliation{\textsuperscript{4}\textit{Exascale Computing Lab, Intel, Paris, France}}
\affiliation{\textsuperscript{5}\textit{Department of Psychology and eScience Institute, University of Washington, Seattle, WA, USA}}
\affiliation{\textsuperscript{6}\textit{Parietal project-team, INRIA Saclay-ile de France, France}}
\affiliation{$\dagger$Authors contributed equally}

\Keywords{Stability --- Reproducibility --- Network Neuroscience --- Neuroimaging} % Keywords - if you don't want any simply remove all the text between the curly brackets
\newcommand{\keywordname}{Keywords} % Defines the keywords heading name

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{The analysis of brain-imaging data requires complex processing pipelines to support findings on brain
function or pathologies. Recent work has shown that variability in analytical decisions, small amounts of noise,
or computational environments can lead to substantial differences in the results, endangering the trust in
conclusions. We explored the instability of results by instrumenting a \newtwo{structural} connectome estimation pipeline with Monte Carlo
Arithmetic to introduce random noise throughout. We evaluated the reliability of the connectomes, \newtwo{the robustness
of} their features, and the \newtwo{eventual} impact on analysis. The stability of results was found to range from
perfectly stable \newtwo{(i.e. all digits of data significant)} to highly unstable \newtwo{(i.e. $0-1$ significant digits)}.
This paper highlights the potential of leveraging induced variance in estimates of brain connectivity to reduce the
bias in networks \newtwo{without compromising reliability,} alongside increasing the robustness \new{and potential upper-bound}
of their applications in the classification of individual differences. We demonstrate that stability evaluations are
necessary for understanding error inherent to brain imaging experiments, and how numerical analysis can be applied to
typical analytical workflows both in brain imaging and other domains of computational science\newtwo{s, as the techniques
used were data and context agnostic and globally relevant}. Overall, while the extreme variability in results due to analytical instabilities
could severely hamper our understanding of brain organization, it also \new{affords us the opportunity to} increase the
robustness of findings.}

%----------------------------------------------------------------------------------------

\newcommand{\new}[1]{{#1}}
\newcommand{\newtwo}[1]{{#1}}
%\newcommand{\newtwo}[1]{{\color{blue}#1\color{black}}}

\begin{document}

\flushbottom % Makes all text pages the same height
\maketitle % Print the title and abstract box
% \tableofcontents % Print the contents section
\thispagestyle{empty} % Removes page numbering from the first page

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\onecolumn

\section*{Introduction}
The modelling of brain networks, called connectomics, has shaped our understanding of the structure and function
of the brain across a variety of organisms and scales over the last
decade~\cite{behrens2012human,xia2016connectomic,morgan2013not,van2016comparative,Rubinov2010-fh,Dubois2016-yr}.
In humans, these wiring diagrams are obtained \textit{in vivo} through Magnetic Resonance Imaging (MRI), and show
promise towards identifying biomarkers of disease. This can not only improve understanding of so-called
``connectopathies'', such as Alzheimer's Disease and Schizophrenia, but potentially pave the way for
therapeutics~\cite{fornito2015connectomics,deco2014great,xie2012mapping,filippi2013assessment,van2014brain}.

However, the analysis of brain imaging data relies on complex computational methods and software. Tools are
trusted to perform everything from pre-processing tasks to downstream statistical evaluation. While these tools
undoubtedly undergo rigorous evaluation on bespoke datasets, in the absence of ground-truth this is often evaluated
through measures of reliability~\cite{Bartko1966-tl,Brandmaier2018-tk,bridgeford2020elim,Kiar2018-jt,antonakakis2020inter}, proxy outcome
statistics, or agreement with existing theory. Importantly, this means that tools \newtwo{and datasets} are not necessarily of known or
consistent quality, and it is not uncommon that equivalent experiments may lead to diverging
conclusions~\cite{botvinik2020variability,bennet2009neural,eklund2016cluster,Kiar2020-lb,Lewis2017-ll,Glatard2015-vc,salari2020file}. While many scientific
disciplines suffer from a lack of reproducibility~\cite{baker20161}, this was recently explored in brain imaging by a
$70$ team consortium which performed equivalent analyses and found widely inconsistent
results~\cite{botvinik2020variability}, and it is likely that software instabilities played a role. \newtwo{This study
does not broach dataset differences, but there are considerable works which demonstrate that data selection may compound
these effects(e.g. ~\cite{antonakakis2020inter,bridgeford2020elim})}.

The present study approached evaluating reproducibility from a computational perspective in which a series of brain
imaging studies were numerically perturbed \new{in} such \new{a way} that the plausibility of results was not affected, and the
implications of the observed instabilities \new{on downstream analyses} were quantified. We accomplished this through the use of Monte Carlo
Arithmetic (MCA)~\cite{Parker1997-qq,Denis2016-wo}, a technique which enables characterization of the sensitivity of a system to
small \new{numerical} perturbations. \new{This is importantly distinct from data perturbation experiments where the underlying
datasets are manipulated or pathologies may be simulated, and allows for the evaluation of experimental uncertainty in real-world
settings.} We explored the impact of \new{numerical} perturbations through the direct comparision of structural connectomes,
the consistency of their features, and their eventual application in a neuroscience study. \new{We also characterized the}
consequences \new{of instability in these pipelines on the reliability of derived datasets,} and \new{discuss how the induced
variability may be harnessed to increase the discriminability of datasets, in an approach akin to ensemble learning}.
\new{Finally, we} make recommendations for the roles \new{perturbation} analyses may play in brain imaging research \new{and beyond}.

%------------------------------------------------
\section*{Results}
\subsection*{Graphs Vary Widely With Perturbations}
\begin{figure*}[hbt]\centering
\includegraphics[width=0.98\linewidth]{figures/fig1_absolute_differences.pdf}
\caption{Exploration of perturbation-induced deviations from reference \newtwo{structural} connectomes.
(\textbf{A}) The absolute deviations \new{between connectomes}, in the form of normalized percent deviation from
reference. \new{The difference in MCA-perturbed connectomes is} shown as the across MCA series, \new{and is presented}
relative to \new{the variability observed} across subsamples, sessions, and subjects.
(\textbf{B}) The number of significant decimal digits in each set of connectomes as obtained \new{by} evaluating the
\new{complete distribution of networks}. In the case of 16, values can be fully relied upon, whereas in the case of 1 only the first
digit of a value can be trusted. \new{Dense} and \new{sparse} perturbations are shown on the left and right, respectively.}
\label{fig:absolute}
\end{figure*}

Prior to exploring the analytic impact of instabilities, a direct understanding of the induced variability was
required. A subset of the Nathan Kline Institute Rockland Sample (NKIRS) dataset~\cite{Nooner2012-eg} was randomly
selected to contain $25$ individuals with two sessions of imaging data, each of which was subsampled into two
components, resulting in four \new{samples} per individual \new{and $100$ samples total ($25 \times 2 \times 2$ samples)}. Structural connectomes were generated with canonical
deterministic and probabilistic pipelines~\cite{Garyfallidis2014-ql,Garyfallidis2012-gg} which were instrumented with
MCA, replicating computational noise either \new{sparsely} or \new{densely} throughout the pipelines~\cite{Denis2016-wo,Kiar2020-lb}.
\new{In the sparse case, a small subset of the libraries were instrumented with MCA, allowing for the evaluation of the
cascading effects of numerical instabilities that may arise. In the dense case, operations are more uniformly perturbed
and thus the law of large numbers suggests that perturbations will quickly offset one-another and only dramatic local
instabilities will have propagating effects. Importantly, the perturbations resulting from the sparse setting
represent a strict subset of the possible outcomes of the dense implementation. The random perturbations are statistically independent from one another
across both settings and simulations. Instrumenting pipelines with MCA increases their computation time, in this
case by multiplication factors of $1.2 \times$ and $7 \times$ for the sparse and dense settings, respectively~\cite{Kiar2020-lb}.
The results obtained were compared to unperturbed (e.g. reference) connectomes in both cases.}
The \new{connectomes} were sampled \new{$20$} times per \new{sample} and once without perturbations, resulting in a total of
$8,400$ connectomes. \new{Two versions of the unperturbed connectomes were generated and compared such that the absence
of variability aside from that induced via MCA could be confirmed}.

The stability of \newtwo{structural} connectomes was evaluated through the \new{normalized percent} deviation from reference~\cite{Kiar2020-lb} and the number
of significant digits
(Figure~\ref{fig:absolute}). The comparisons were grouped according to differences across simulations, subsampling
of data, sessions of acquisition, or subjects, \new{and accordingly sorted from most to least similar}. While the
similarity of connectomes decreases as the collections become
more distinct, connectomes generated with \new{sparse} perturbations show considerable variability, often reaching deviations
equal to or greater than those observed across individuals or sessions (Figure~\ref{fig:absolute}A; right).
\new{Interpretting these results with respect to the distinct MCA environments used suggests that the tested pipelines
may not suffer from single dominant sources of instability, but that nevertheless there exist minor local instabilities
which may the propagate throughout the pipeline. Furthermore,} this
finding suggests that instabilities inherent to these pipelines may mask session or individual differences, limiting
the trustworthiness of derived connectomes. While both pipelines show similar performance, the probabilistic pipeline
was more stable in the face of \new{dense} perturbations whereas the deterministic was more stable to \new{sparse} perturbations
($p < 0.0001$ for all; exploratory). \new{As an alternative to the normalized percent deviation, the stability of}
correlations \new{between} networks can be found in \sref{supsec:correlation}.

The number of significant digits per edge across connectomes (Figure~\ref{fig:absolute}B) similarly decreases
\new{alongside the decreasing similarity between comparison groups}. While the cross-MCA comparison of connectomes
generated with \new{dense} perturbations show nearly perfect
precision for many edges (approaching the maximum of $15.7$ digits for $64$-bit data), this evaluation uniquely shows
considerable drop off in performance \new{when comparing networks across} subsamplings (average of $< 4$ digits). In addition, \new{sparsely
perturbed connectomes} show no more than an average of $3$ significant digits across all \new{comparison} groups, demonstrating a significant
limitation in the reliability \new{of} independent edge weights. \new{The number of significant digits} across individuals did not exceed a single digit
per edge in any case, indicating that only the order \new{of} magnitude of edges in naively computed groupwise average connectomes can
be trusted. The combination of these results with those presented in Figure~\ref{fig:absolute}A suggests that while
specific edge weights are largely affected by instabilities, macro-scale network \new{structure} is stable.

\subsection*{Sparse Perturbations Reduce Off-Target Signal}
\begin{table*}[ht]\centering
\caption{The impact of instabilities as evaluated through the discriminability of the dataset based on individual (or
subject) differences, session, and subsample. The performance is reported as mean discriminability. While a perfectly
discriminable dataset would be represented by a score of $1.0$, the chance performance, indicating minimal discriminability, is
$1 /$the number of classes. $H_3$ could not be tested using the reference executions due to too few possible
comparisons. The alternative hypothesis, indicating significant discrimination, was accepted for all experiments, with
$p < 0.005$ \newtwo{after correcting for multiple comparisons~\cite{}}.}
\vspace{5pt}
\input{figures/tab1_discrim.tex}
\label{tab:discrim}
\end{table*}

We assessed the reproducibility of the dataset through mimicking and extending a typical test-retest
experiment~\cite{bridgeford2020elim} in which the similarity of samples across sessions were
compared to distinct samples in the dataset (Table~\ref{tab:discrim}, with additional experiments and explanation of
the measure and its scaling in \sref{supsec:discrimfull}). The ability to discriminate connectomes across subjects
(Hypothesis~1) is an essential prerequisite for the application of brain imaging towards identifying individual
differences~\cite{Dubois2016-yr}. In testing hypothesis~1, we observe that the dataset is discriminable with a scaled
score of $0.82$ ($p < 0.001$; optimal score: $1.0$; chance: $0.04$) for both pipelines in the absence of MCA. We can
see that inducing instabilities through MCA preserves the discriminability in the dense perturbtion setting, and
and discriminability decreased slightly but remained above the unscaled reference value of $0.65$ in the sparse case.
This lack of significant decrease in discriminability across MCA perturbations suggests its utility for capturing
variance within datasets without compromising the robustness and reliability of the dataset as a whole, and possibly
suggests this technique as a cost effective and context-agnostic method for dataset augmentation.

While the discriminability of individuals is essential for the identification of individual brain networks, it is
similarly reliant on network similarity – or lack of discriminability – across equivalent acquisitions (Hypothesis~2).
In this case, connectomes were grouped based upon session, rather than subject, and the ability to distinguish one
session from another based on subsamples was computed within-individual and aggregated. Both the unperturbed and dense
perturbation settings perfectly preserved differences between sessions with a score of $1.0$ ($p < 0.005$; optimal
score: $0.5$; chance: $0.5$), indicating a dominant session-dependent signal for all individuals despite no intended
biological differences. However, while still significant relative to chance (score: $0.85$ and $0.88$; $p < 0.005$ for
both), sparse perturbations lead to significantly lower discriminability of the dataset ($p < 0.005$ for all).
This reduction of the difference between sessions suggests that the added variance due to perturbations
reduces the relative impact of non-biological acquisition-dependent bias inherent in the networks.

Though the previous sets of experiments inextricably evaluate the interaction between data acquisition and tool,
the use of subsampling allowed for characterizing the discriminability of networks sampled from within a single
acquisition (Hypothesis~3). While this experiment could not be evaluated using reference executions, the networks
generated with dense perturbations showed near perfect discrimination between subsamples, with scores of $0.99$ and
$1.0$ ($p < 0.005$; optimal: $0.5$; chance: $0.5$). Given that there was no variability in data acquisition, due to
undesired effects such as participant motion, or preprocessing, the ability to discriminate between equivalent
subsamples in this experiment may only be due to instability or bias inherent to the pipelines. The high variability
introduced through sparse perturbations considerably lowered the discriminability towards chance (score:
$0.71$ and $0.61$; $p < 0.005$ for all), further supporting this as an effective method for obtaining lower-bias
estimates of individual connectivity.

Across all cases, the induced perturbations maintained the ability to discriminate networks on the basis of
meaningful biological signal alongside a reduction in discriminability due to of off-target signal in the sparse
perturbation setting. This result appears strikingly like a manifestation of the well-known bias-variance
tradeoff~\cite{geman1992neural} in machine learning, a concept which observes a decrease in bias as variance is
favoured by a model. In particular, this highlights that numerical perturbations can be used to not only evaluate the
stability of pipelines, but that the induced variance may be leveraged for the interpretation as a robust distribution
of possible results.

\subsection*{Distributions of Graph Statistics Are Reliable, But Individual Statistics Are Not}
\begin{figure*}[bht!]\centering
\includegraphics[width=\linewidth]{figures/fig2_multivariate_differences.pdf}
\caption{Distribution and stability assessment of multivariate graph statistics. (\textbf{A}, \textbf{B}) The
cumulative distribution functions of multivariate statistics across all subjects and perturbation settings. There was
no significant difference between the distributions in A and B. (\textbf{C}, \textbf{D}) The number of significant
digits in the first $5$ five moments of each statistic across perturbations. The dashed red line refers to the maximum
possible number of significant digits.}
\label{fig:multivar}
\end{figure*}

Exploring the stability of topological features of \newtwo{structural} connectomes is relevant for typical analyses, as low dimensional
features are often more suitable than full connectomes for many analytical methods in practice~\cite{Rubinov2010-fh}.
A separate subset of the NKIRS dataset was randomly selected to contain a single non-subsampled session for $100$
individuals \new{($100 \times 1 \times 1$) using the pipelines and instrumentation methods to generate connectomes as
above. Connectomes were generated $20$ times each, resulting in a dataset which also contained $8,400$ connectomes with
the MCA simulations serving as the only source of repeated measurements.}

The stability of several commonly-used multivariate graph features~\cite{Betzel2018-eo} \new{were} explored \new{and
are presented} in Figure~\ref{fig:multivar}. The cumulative density of the features was computed within individuals and
the mean \new{cumulative} density and associated standard error were computed for across individuals
(Figures~\ref{fig:multivar}A and \ref{fig:multivar}B). There was no significant difference between the distributions
for each feature across the two perturbation settings, suggesting that the topological features summarized by these
multivariate features are robust across both perturbation modes.

In addition to the comparison of distributions, the stability of the first $5$ moments of these features was evaluated
(Figures~\ref{fig:multivar}C and \ref{fig:multivar}D). In the face of \new{dense} perturbations, the feature-moments
were stable with more than $10$ significant digits with the exception of edge weight when using the deterministic
pipeline, though the probabilistic pipeline was more stable for all comparisons ($p < 0.0001$; exploratory). In stark
contrast, \new{sparse} perturbations led to highly unstable feature-moments (Figure~\ref{fig:multivar}D), such that
none contained more than $5$ significant digits of information and several contained less than a single significant
digit, indicating a complete lack of reliability. This dramatic degradation in stability for individual measures
strongly suggests that these features may be unreliable as individual biomarkers when derived from a single pipeline
evaluation, though their reliability may be increased when studying their distributions across perturbations. A similar
analysis was performed for univariate statistics \new{which obtained similar findings} and can be found in
\sref{supsec:univar}.

\subsection*{Uncertainty in Brain-Phenotype Relationships}

\begin{figure*}[ht]\centering
\includegraphics[width=0.7\linewidth]{figures/fig3_bmi_classification.pdf}
\caption{Variability in BMI classification across the sampling of an MCA-perturbed dataset. The dashed red lines
indicate random-chance performance, and the orange dots show the performance using the reference executions.}
\label{fig:bmi}
\end{figure*}
 
While the variability of connectomes and their features was summarized above, networks are commonly used as inputs to
machine learning models tasked with learning brain-phenotype relationships~\cite{Dubois2016-yr}. To explore the
stability of these analyses, we modelled the relationship between high- or low- Body Mass Index (BMI) groups and brain
connectivity using standard dimensionality reduction and classification tools. \newtwo{In particular, we used Principal
Component Analysis followed by a Logistic Regression classifier to predict BMI label, and demonstrated similar performance
to previous work which adopted similar techniques for this task~\cite{Park2015-uj,Gupta2015-ap}. We}
compared \newtwo{the performance achieved across numerically perturbed samples to both the} reference and random
performance (Figure~\ref{fig:bmi}).

The analysis was perturbed through distinct samplings of the dataset across both pipelines and perturbation methods.
The accuracy and F1 score for the perturbed models varied from $0.520$~–~$0.716$ and $0.510$~–~$0.725$, respectively,
ranging from at or below random performance to outperforming performance on the reference dataset. This large
variability illustrates a previously uncharacterized margin of uncertainty in the modelling of this relationship, and
limits confidence in reported accuracy scores on singly processed datasets. The portion of explained variance in these
samples ranged from $88.6\%$~-–~$97.8\%$, similar to the reference \new{of $90.3\%$}, suggesting that the range in
performance was not due to a gain or loss of meaningful signal, but rather the reduction of bias towards specific
outcome. Importantly, this finding does not suggest that modelling brain-phenotype relationships is not possible, but
rather it sheds light on impactful uncertainty that must be accounted for in this process, and supports the use of
ensemble modeling techniques.

\new{One distinction between the results presented here and the previous is that while networks derived from dense
perturbations had been shown to exhibit less dramatic instabilities in general, the results here show similar
variability in classification performance across the two methods. This consistency suggests that the desired method of
instrumentation may vary across experiments. While sparse perturbations result in considerably more
variability in networks directly, the two techniques capture similar variability when relating networks to this
phenotypic variable. Given the dramatic reduction in computational overhead, a sparse instrumentation may be preferred
when processing datasets for eventual application in modelling brain-phenotype relationships.}

\section*{Discussion}

The perturbation of structural connectome estimation pipelines with small amounts of noise, on the order of machine
error, led to considerable variability in derived brain graphs. Across all analyses the stability of results ranged
from nearly perfectly trustworthy (i.e. no variation) to completely unreliable (i.e. containing no trustworthy
information). Given that the magnitude of introduced numerical noise is to be expected in computational workflows, this
finding has potentially significant implications for inferences in brain imaging as it is currently performed. In
particular, this bounds the success of studying individual differences, a central objective in brain
imaging~\cite{Dubois2016-yr}, given that the quality of relationships between phenotypic data and brain networks will
be limited by the stability of the connectomes themselves. This issue is accentuated through the crucial finding that
individually derived network features were unreliable despite there being no significant difference in their aggregated
distributions. This finding is not damning for the study of brain networks as a whole, but rather is strong support for
the aggregation of networks, either across perturbations for an individual or across groups, over the use of individual
estimates.

\paragraph{Underestimated False Positive Rates}
While the instability of brain networks was used here to demonstrate the limitations of modelling brain-phenotype
relationships in the context of machine learning, this limitation extends to classical hypothesis testing, as well.
Though performing individual comparisons in a hypothesis testing framework will be accompanied by reported false
positive rates, the accuracy of these rates is critically dependent upon the reliability of the samples used. In
reality, the true false positive rate for a test would be a combination of the reported confidence and the underlying
variability in the results, a typically unknown quantity.

When performing these experiments outside of a repeated-measure context, such as that afforded here through MCA, it is
impossible to empirically estimate the reliability of samples. This means that the reliability of accepted hypotheses
is also unknown, regardless of the reported false positive rate. In fact, it is a virtual certainty that the true false
positive rate for a given hypothesis exceeds the reported value simply as a result of numerical instabilities. This
uncertainty inherent to derived data is compounded with traditional arguments limiting the trustworthiness of
claims~\cite{ioannidis2005most}, and hampers the ability of researchers to evaluate the quality of results. The
accompaniment of brain imaging experiments with direct evaluations of their stability, as was done here, would allow
researchers to simultaneously improve the numerical stability of their analyses and accurately gauge confidence in
them. The induced variability in derived brain networks may be leveraged to estimate aggregate connectomes with lower
bias than any single independent observation, leading to learned relationships that are more generalizable and
ultimately more useful.

\paragraph{Cost-Effective Data Augmentation}
The evaluation of reliability in brain imaging has historically relied upon the expensive collection of repeated
measurements choreographed by massive cross-institutional consortia~\cite{van2013wu,zuo2014open}. The finding that
perturbing experiments using MCA both preserved the discriminability of the dataset due to biological signal
and decreased the discriminability due to off-target differences across acquisitions and subsamples opens
the door for a promising paradigm shift. Given that MCA is data-agnostic, this technique
could be used effectively in conjunction with, or in lieu of, realistic noise models to augment existing datasets.
While this of course would not replace the need for repeated measurements when exploring the effect of data collection
paradigm or study longitudinal progressions of development or disease, it could be used in conjunction with these
efforts to decrease the bias of each distinct sample within a dataset. In contexts where repeated measurements
are typically collected to increase the fidelity of the dataset, MCA could potentially serve as an alternative
solution to capture more biological variability, with the added benefit being the savings of millions of dollars on
data collection. 

\paragraph{Shortcomings and Future Questions}
Given the complexity of recompiling complex software libraries, pre-processing was not perturbed in these experiments
\new{as the instrumentation of the canonical workflow used in diffusion image processing would have added considerable
technical complexity and computational overhead to the large set of experiments performed here}\newtwo{; similarly, this
complexity along with the added layer of difficulty in comparing instrumentations meant that only algorithms within a
single library were tested}.
Other work has shown that linear registration, a core piece of many elements of pre-processing such as motion
correction and alignment, is sensitive to minor perturbations~\cite{Glatard2015-vc}. It is likely that the
instabilities across the entire processing workflow would be compounded with one another, resulting in even greater
variability. While the analyses performed in this paper evaluated a single dataset and set of pipelines, extending this
work to other modalities and analyses, \new{alongside the detection of local sources of instability within pipelines},
is of interest for future projects.

This paper does not explore methodological flexibility or compare this to numerical instability. Recently, the nearly
boundless space of analysis pipelines and their impact on outcomes in brain imaging has been clearly
demonstrated~\cite{botvinik2020variability}. The approach taken in these studies complement one another and explore
instability at the opposite ends of the spectrum, with human variability in the construction of an analysis workflow on
one end and the unavoidable error implicit in the digital representation of data on the other. It is of extreme
interest to combine these approaches and explore the interaction of these scientific degrees of freedom with effects
from software implementations, libraries, and parametric choices.

Finally, it is important to state explicitly that the work presented here does not invalidate analytical pipelines used
in brain imaging, but merely sheds light on the fact that many studies are accompanied by an unknown degree of
uncertainty due to machine-introduced errors. The presence of unknown error-bars associated with experimental findings
limits the impact of results due to increased uncertainty. The desired outcome of this paper is to motivate a shift in
scientific computing – both in neuroimaging and more broadly – towards a paradigm \new{that} favours the explicit evaluation
of the trustworthiness of claims alongside the claims themselves.

\newpage
\section*{Materials \& Methods}

\subsection*{Dataset}
The Nathan Kline Institute Rockland Sample (NKI-RS)~\cite{Nooner2012-eg} dataset contains high-fidelity imaging and
phenotypic data from over $1,000$ individuals spread across the lifespan. A subset of this dataset was chosen for each
experiment to both match sample sizes presented in the original analyses and to minimize the computational burden of
performing MCA. The selected subset comprises $100$ individuals ranging in age from $6$ – $79$ with a mean of $36.8$
(original: $6$ – $81$, mean $37.8$), $60\%$ female (original: $60\%$), with $52\%$ having a BMI over $25$ (original:
$54\%$).

Each selected individual had at least a single session of both structural T1-weighted (MPRAGE) and diffusion-weighted
(DWI) MR imaging data. DWI data was acquired with $137$ diffusion directions in a single shell; more information regarding the
acquisition of this dataset can be found in the NKI-RS data release~\cite{Nooner2012-eg}.

In addition to the $100$ sessions mentioned above, $25$ individuals had a second session to be used in a test-retest
analysis. Two additional copies of the data for these individuals were generated, including only the odd or even
diffusion directions ($64$ + $9$ B0 volumes = $73$ in either case) such that the acquired data was evenly
represented across both portions, and each subsample represented a realistic complete acquisition. This allowed for
an extra level of stability evaluation to be performed between the levels of MCA and session-level variation.

In total, the dataset is composed of $100$ subsampled sessions of data originating from $50$ acquisitions
and $25$ individuals for in depth stability analysis, and an additional $100$ sessions of full-resolution data from
$100$ individuals for subsequent analyses.


\subsection*{Processing}
The dataset was preprocessed using a standard FSL~\cite{Jenkinson2012-ly} workflow consisting of eddy-current
correction and alignment. The MNI152 atlas~\cite{lancaster2007bias} was aligned to each session of data via the
structural images, and the resulting transformation was applied to the DKT parcellation~\cite{Klein2012-vi}.
Subsampling the diffusion data took place after preprocessing was performed on full-resolution sessions, ensuring that
an additional confound was not introduced in this process when comparing between downsampled sessions. The preprocessing
described here was performed once without MCA, and thus is not being evaluated.

Structural connectomes were generated from preprocessed data using two canonical pipelines from
Dipy~\cite{Garyfallidis2014-ql}: deterministic and probabilistic. In the deterministic pipeline, a constant solid angle
model was used to estimate tensors at each voxel and streamlines were then generated using the EuDX
algorithm~\cite{Garyfallidis2012-gg}. In the probabilistic pipeline, a constrained spherical deconvolution model was
fit at each voxel and streamlines were generated by iteratively sampling the resulting fiber orientation distributions.
In both cases tracking occurred with $8$ seeds per 3D voxel and edges were added to the graph based on the location of
terminal nodes with weight determined by fiber count.

The random state of both pipelines was fixed for all analyses. Fixing this random state led to entirely
deterministic repeated-evaluations of the tools, and allowed for explicit attribution of observed variability to
limitations in tool precision as provoked by Monte Carlo simulations, rather than the internal state of the algorithm.

\subsection*{Perturbations}
All connectomes were generated with one reference execution where no perturbation was introduced in the processing. For
all other executions, all floating point operations were instrumented with Monte Carlo Arithmetic
(MCA)~\cite{Parker1997-qq} through Verificarlo~\cite{Denis2016-wo}. MCA simulates the distribution of errors implicit
to all instrumented floating point operations (flop). This rounding is performed on a value $x$ at precision $t$ by:

\begin{equation}
inexact(x) = x + 2^{e_x - t}\xi 
\label{eq:inexact}
\end{equation}

where $e_x$ is the exponent value of $x$ and $\xi$ is a uniform random variable in the range ($-\frac{1}{2}$,
$\frac{1}{2}$). MCA can be introduced in two places for each flop: before or after evaluation. Performing MCA on the
inputs of an operation limits its precision, while performing MCA on the output of an operation highlights round-off
errors that may be introduced. The former is referred to as Precision Bounding (PB) and the latter is called Random
Rounding (RR).

Using MCA, the execution of a pipeline may be performed many times to produce a distribution of results. Studying the
distribution of these results can then lead to insights on the stability of the instrumented tools or functions. To
this end, a complete software stack was instrumented with MCA and is made available on GitHub at
\url{https://github.com/verificarlo/fuzzy}.

The RR variant of MCA was used for all experiments. As was presented in~\cite{Kiar2020-lb}, both the degree of
instrumentation (i.e. number of affected libraries) and the perturbation mode have an effect on the distribution of
observed results. For this work, the RR-MCA was applied across the bulk of the relevant operations (those occurring
in BLAS, LAPACK, Python, Cython, and Numpy) and is referred to as dense perturbation. In this case the bulk of
numerical operations were affected by MCA.

Conversely, the case in which RR-MCA was applied across the operations in a small subset of operations (those ocurring
in Python and Cython) is here referred to as sparse perturbation. In this case, the inputs to operations within the
instrumented libraries were perturbed, resulting in less frequent, data-centric perturbations. Alongside the stated theoretical
differences, sparse perturbation is considerably less computationally expensive than dense perturbation.

All perturbations targeted the least-significant-bit for all data ($t=24$ and $t=53$ in float32 and float64,
respectively~\cite{Denis2016-wo}). Perturbing the least significant bit importantly serves as a perturbation of
machine error, and thus is the appropriate precision to be applied globally in complex pipelines. Simulations were
performed $20$ times for each pipeline execution for the $100$ sample dataset and $10$ times for the repeated
measures dataset. A detailed motivation for the number of simulations can be found in~\cite{Sohier2018-ts}.

\subsection*{Evaluation}

The magnitude and importance of instabilities in pipelines can be considered at a number of analytical levels, namely:
the induced variability of derivatives directly, the resulting downstream impact on summary statistics or features, or
the ultimate change in analyses or findings. We explore the nature and severity of instabilities through each of these
lenses. Unless otherwise stated, all p-values were computed using Wilcoxon signed-rank tests \newtwo{and corrected for multiple comparisons}. \new{To avoid biasing
these statistics in this unique repeated-measures context, tests were performed across sets of independent
observations and then the results were aggregated in all cases.}

\subsubsection*{Direct Evaluation of the Graphs}

The differences between \new{perturbation-generated} graphs was measured directly through both a direct variance quantification and a
comparison to other sources of variance such as individual- and session-level differences.

\paragraph{Quantification of Variability}
Graphs, in the form of adjacency matrices, were compared to one another using three metrics: normalized percent
deviation, Pearson correlation, and edgewise significant digits. The normalized percent deviation measure, defined
in~\cite{Kiar2020-lb}, scales the norm of the difference between a simulated graph and the reference execution (that
without intentional perturbation) with respect to the norm of the reference graph, \new{and is defined as~\cite{Kiar2020-lb}:

\begin{equation}
\% Dev (A, B) = \sqrt{\sum_{i=1}^m\sum_{j=1}^n \lvert a_{ij} - b_{ij} \rvert^2 } / \sqrt{\sum_{i=1}^m\sum_{j=1}^n \lvert a_{ij} \rvert^2},
\label{eq:eval}
\end{equation}

where $A$ and $B$ each represent a graph, and $\square_{ij}$ are elements therein corresponding to row and column
$i$ and $j$, respectively. For these experiments, the $A$ graph always refers to the reference, where $B$ represents a
perturbed value}. The purpose of this comparison is
to provide insight on the scale of differences in observed graphs relative to the original signal intensity. A Pearson
correlation coefficient~\cite{Benesty2009-cb} was computed in complement to normalized percent deviation to identify
the consistency of structure and not just intensity between observed graphs, \new{though the result of this experiment
is shown only in \sref{supsec:correlation}}.

Finally, the estimated number of significant digits, $s'$, for each edge in the graph is calculated as:

\begin{equation}
s' = -log_{10}\frac{\sigma}{\lvert\mu\rvert}
\label{eq:sigdig}
\end{equation}

where $\mu$ and $\sigma$ are the mean and unbiased estimator of standard deviation across graphs, respectively. The
upper bound on significant digits is $15.7$ for 64-bit floating point data.

The percent deviation, correlation, and number of significant digits were each calculated within a single session of
data, thereby removing any subject- and session-effects and providing a direct measure of the tool-introduced
variability across perturbations. A distribution was formed by aggregating these individual results.

\paragraph{Class-based Variability Evaluation} To gain a concrete understanding of the significance of observed
variations we explore the separability of our results with respect to understood sources of variability, such as 
subject-, session-, and pipeline-level effects. This can be probed through Discriminability~\cite{bridgeford2020elim},
a technique similar to ICC~\cite{Bartko1966-tl} which relies on the mean of a ranked distribution of distances between
observations belonging to a defined set of classes. The discriminability statistic is formalized as follows:

\begin{equation}
Disc. = Pr(\lVert g_{ij} - g_{ij'} \rVert \leq \lVert g_{ij} - g_{i'j'} \rVert)
\label{eq:sigdig}
\end{equation}

where $g_{ij}$ is a graph belonging to class $i$ that was measured at observation $j$, where $i \neq i'$ and
$j \neq j'$.

Discriminability can then be read as the probability that an observation belonging to a given class will be more
similar to other observations within that class than observations of a different class. It is a measure of
reproducibility, and is discussed in detail in~\cite{bridgeford2020elim}. This definition allows for the exploration of
deviations across arbitrarily defined classes that in practice can be any of those listed above. We combine this
statistic with permutation testing to test hypotheses on whether differences between classes are statistically
significant in each of these settings. \new{This statistic is similar to $ICC$~\cite{Bartko1966-tl} in a two-measurement
setting, however, given the dependence on a rank distribution from all measurements, discriminability scores do not
become meaningless by the addition of more samples which are highly similar to the originals, whereas ICC scores would
much more rapidly trend towards 1, making discriminability appropriate in this context.} The scaling properties of 
discriminability are described more fully in~\sref{supsec:discrimfull}.

With this in mind, three hypotheses were defined. For each setting, we state the alternate hypotheses, the variable(s)
which were used to determine class membership, and the remaining variables which may be sampled when obtaining multiple
observations. Each hypothesis was tested independently for each pipeline and perturbation mode.

\begin{enumerate}[label=$H_{A\arabic*}$:]
\item \underline{Individuals are distinct from one another}\\
Class definition: \textit{Subject ID}\\
Comparisons: \textbf{\textit{Session (1 subsample)}}, \textit{Subsample (1 session)},
\textit{MCA (1 subsample, 1 session)}
\item \underline{Sessions within an individual are distinct}\\
Class definition: \textit{Session ID $\vert$ Subject ID}\\
Comparisons: \textbf{\textit{Subsample}}, \textit{MCA (1 subsample)}
\item \underline{Subsamples are distinct}\\
Class definition: \textit{Subsample $\vert$ Subject ID, Session ID}\\
Comparisons: \textbf{\textit{MCA}}
\end{enumerate}

As a result, we tested $3$ hypotheses across $6$ MCA experiments and $3$ reference experiments on $2$ pipelines and $2$
perturbation modes, resulting in a total of $30$ distinct tests. \new{While results from all tests can be found
within~\sref{supsec:discrimfull}, only the bolded comparisons in the list above have been presented in the main body of
this article. Correction for repeated testing was performed.}

\subsubsection*{Evaluating Graph-Theoretical Metrics}
While connectomes may be used directly for some analyses, it is common practice to summarize them with structural
measures, that can then be used as lower-dimensional proxies of connectivity in so-called graph-theoretical
studies~\cite{Rubinov2010-fh}. We explored the stability of several commonly-used univariate (graphwise) and
multivariate (nodewise or edgewise) features. The features computed and subsequent methods for comparison in this
section were selected to closely match those computed in~\cite{Betzel2018-eo}.

\paragraph{Univariate Differences} For each univariate statistic (edge count, mean clustering coefficient, global
efficiency, modularity of the largest connected component, assortativity, and mean path length) a distribution of
values across all perturbations within subjects was observed. A Z-score was computed for each sample with respect to
the distribution of feature values within an individual, and the proportion of "classically significant" Z-scores, i.e.
corresponding to $p < 0.05$, was reported and aggregated across all subjects. \new{There was no correction for multiple
comparisons in these statistics, as they were not used to interpret a hypothesis but demonstrate the false-positive
rate due to perturbations.} The number of significant digits
contained within an estimate derived from a single subject were calculated and aggregated. \new{The results of this
analysis can be found in \sref{supsec:univar}}.

\paragraph{Multivariate Differences} In the case of both nodewise (degree distribution, clustering coefficient,
betweenness centrality) and edgewise (weight distribution, connection length) features, the cumulative density
functions of their distributions were evaluated over a fixed range and subsequently aggregated across individuals. The
number of significant digits for each moment of these distributions (sum, mean, variance, skew, and kurtosis) were
calculated across observations within a sample and aggregated.

\subsubsection*{Evaluating A Brain-Phenotype Analysis}
Though each of the above approaches explores the instability of derived connectomes and their features, many modern
studies employ modeling or machine-learning approaches, for instance to learn brain-phenotype relationships or identify
differences across groups. We carried out one such study and explored the instability of its results with respect to
the upstream variability of connectomes characterized in the previous sections. We performed the modeling task with a
single sampled connectome per individual and repeated this sampling and modelling 20 times. We report the model
performance for each sampling of the dataset and summarize its variance.

\paragraph{BMI Classification} Structural changes have been linked to obesity in adolescents and
adults~\cite{Raji2010-lh}. We classified normal-weight and overweight individuals from their structural networks (using
for overweight a cutoff of BMI $> 25$~\cite{Gupta2015-ap}). We reduced the dimensionality of the connectomes through
principal component analysis (PCA), and provided the first N-components to a logistic regression classifier for
predicting BMI class membership, similar to methods shown in~\cite{Gupta2015-ap,Park2015-uj}. The number of components
was selected as the minimum set which explained $> 90\%$ of the variance when averaged across the training set for each
fold within the cross validation of the original graphs; this resulted in a feature of $20$ components. We trained the
model using $k$-fold cross validation, with $k = 2, 5, 10,$ and $N$ (equivalent to leave-one-out; LOO).

\subsubsection*{Data \& Code Provenance}
The unprocessed dataset is available through The Consortium of Reliability and Reproducibility
(\url{http://fcon_1000.projects.nitrc.org/indi/enhanced/}), including both the imaging data as well as phenotypic data
which may be obtained upon submission and compliance with a Data Usage Agreement. The connectomes generated through
simulations have been bundled and stored permanently (\url{https://doi.org/10.5281/zenodo.4041549}), and are made
available through The Canadian Open Neuroscience Platform (\url{https://portal.conp.ca/search}, search term "Kiar").

All software developed for processing or evaluation is publicly available on GitHub at
\url{https://github.com/gkpapers/2021ImpactOfInstability}. Experiments were launched using
Boutiques~\cite{Glatard2018-tu} and Clowdr~\cite{Kiar2019-sr} in Compute Canada's HPC cluster environment. MCA
instrumentation was achieved through Verificarlo~\cite{Denis2016-wo} available on Github at
\url{https://github.com/verificarlo/verificarlo}. A set of MCA instrumented software containers is available on Github
at \url{https://github.com/gkiar/fuzzy}.


\subsection*{Author Contributions}
GK was responsible for the experimental design, data processing, analysis, interpretation, and the majority of writing.
All authors contributed to the revision of the manuscript. YC, POC, and EP were responsible for MCA tool development
and software testing. AR, GV, and BM contributed to experimental design and interpretation. TG contributed to
experimental design, analysis, and interpretation. TG and ACE were responsible for supervising and supporting all
contributions made by GK. The authors declare no competing interests for this work. Correspondence and requests for
materials should be addressed to Tristan Glatard at \url{tristan.glatard@concordia.ca}.

\subsection*{Acknowledgments} 
This work was supported in partnership with Health Canada, for the Canadian Open Neuroscience Platform initiative.

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
% \phantomsection
\bibliographystyle{IEEEtran}
\bibliography{impact-of-instability}

%----------------------------------------------------------------------------------------
\beginsupplement

\clearpage
\section{Graph Correlation}
\label{supsec:correlation}
The following presents a quantification of deviations of generated connectomes from the reference execution, similar
to shown in Figure~\ref{fig:absolute}. However, in this case, the ``percent deviation'' measure was replaced with the
Pearson correlation coefficient. The correlations between observed graphs (Figure~\ref{fig:correlation}) across each
grouping follow the same trend to as percent deviation, as shown in Figure~\ref{fig:absolute}. However, notably
different from percent deviation, there is no significant difference in the correlations between \new{dense} or
\new{sparse} instrumentations. By this measure, the probabilistic pipeline is more stable in all cross-MCA and
cross-directions except for the combination of \new{sparse} perturbation and cross-MCA ($p < 0.0001$ for all;
exploratory).

The marked lack in drop-off of performance across these settings, inconsistent with the measures show in
Figure~\ref{fig:absolute} is \new{likely due} to the nature of the measure and the \new{structure of graphs being compared}.
Given that structural graphs are sparse and contain considerable numbers of zero-weighted edges, the presence or
absense of edges dominated the correlation measure where it was less impactful for the others. For this reason and
others~\cite{huang2016linking}, correlation is not a commonly used measure in the context of structural connectivity\new{,
and thus this analysis was demoted to the supplement material}.

\begin{figure}[ht]\centering
\includegraphics[width=\linewidth]{figures/figS1_correlation_differences.pdf}
\caption{The correlation between perturbed connectomes and their reference.}
\label{fig:correlation}
\end{figure}

\clearpage
\section{Complete Discriminability Analysis}
\label{supsec:discrimfull}

\begin{table}[ht]\centering
\caption{The complete results from the Discriminability analysis, with results reported as mean~$\pm$~standard
deviation Discriminability. As was the case in the condensed table, the alternative hypothesis, indicating significant
separation across groups, was accepted for all experiments, with $p < 0.005$.}
\vspace{5pt}
\input{figures/tabS1_discrim_full.tex}
\label{stab:discrim_full}
\end{table}

The complete discriminability analysis includes comparisons across more axes of variability than the condensed version.
The reduction in the main body was such that only axes which would be relevant for a typical analysis were presented.
Here, each of Hypothesis $1$, testing the difference across subjects, and $2$, testing the difference across sessions,
were accompanied with additional comparisons to those shown in the main body.

\paragraph{Subject Variation}
Alongside experiment $1.1$, that which mimicked a typical test-retest scenario, experiments $1.2$ and $1.3$ could be
considered a test-retest with a handicap, given a single aqcuisition per individual was compared either across
subsamples or simulations, respectively. For this reason, it is unsurprising that the dataset achieved considerably
higher discriminability scores. 

\paragraph{Session Variation}
Similar to subject variation, the session variation was also modelled across either both or a single subsample \new{in
experiments $2.4$ and $2.5$}. In both of these cases the performance was similar, and the finding that \new{sparse}
perturbations reduced the off-target signal was consistent.

\begin{figure}[ht]\centering
\includegraphics[width=0.8\linewidth]{figures/figS2_discrim_scaling.pdf}
\caption{Scaling behaviour of the discriminability statistic with data duplication.}
\label{fig:discrim_scaling}
\end{figure}

\subsection{Scaling of discriminability with N}
When samples were added to the dataset across perturbed executions, the discriminability statistic inflated to a
plateau even when no information was added (e.g. the dataset was replicated). This effect is demonstrated for the
reference executions and is shown in Figure~\ref{fig:discrim_scaling}. As we can see, the reference discriminability
scores without data duplication (unscaled) were $0.64$ and $0.65$ for the deterministic and probabilistic pipelines,
respectively. After duplicating the dataset $20$ times, matching the size of the $20$-sample perturbed dataset, we
can see that this (scaled) score plateaus at $0.82$ for both pipelines. For consistency, in the main body of the text
the reference execution performance was communicated as the scaled quantity.



\clearpage
\section{Univariate Graph Statistics}
\label{supsec:univar}

Figure~\ref{sfig:univariate} explores the stability of univariate graph-theoretical metrics computed from the perturbed
graphs, including modularity, global efficiency, assortativity, average path length, and edge count. When aggregated
across individuals and perturbations, the distributions of these statistics (Figures~\ref{sfig:univariate}A and
\ref{sfig:univariate}2B) showed no significant differences between perturbation methods for either deterministic or
probabilistic pipelines\new{, consistent with the comparison of the cumulative density of the multivariate statistics
compared in \ref{fig:multivar}}.

However, when quantifying the stability of these measures across connectomes derived from a single session of data, the
two perturbation methods show considerable differences. The number of significant digits in univariate statistics for
\new{dense} perturbation instrumented connectome generation exceeded $11$ digits for all measures except modularity, which
contained more than $4$ significant digits of information (Figure~\ref{sfig:univariate}C). When detecting \new{false-positives}
from the distributions of observed statistics for a given session, the rate (using a threshold of $p = 0.05$)
was approximately $2\%$ for all statistics with the exception of modularity which again was less stable with an
approximately $10\%$ false positive rate. The probabilistic pipeline is significantly more stable than the
deterministic pipeline ($p < 0.0001$; exploratory) for all features except modularity. When similarly evaluating these
features from connectomes generated in the \new{sparse} perturbation setting, no statistic was stable with more than $3$
significant digits or a false positive rate lower than nearly $6\%$ (Figure~\ref{sfig:univariate}D). The deterministic
pipeline was more stable than the probabilistic pipeline in this setting ($p < 0.0001$; exploratory).

\begin{figure}[bhtp!]\centering
\includegraphics[width=0.8\linewidth]{figures/figS2_univariate_differences.pdf}
\caption{Distribution and stability assessment of univariate graph statistics. (\textbf{A}, \textbf{B}) The
distributions of each computed univariate statistic across all subjects and perturbations for \new{dense} and \new{sparse}
settings, respectively. There was no significant difference between the distributions in A and B. (\textbf{C},
\textbf{D}; top) The number of significant decimal digits in each statistic across perturbations, averaged across
individuals. The dashed red line refers to the maximum possible number of significant digits.
(\textbf{C}, \textbf{D}; bottom) The percentage of connectomes which were deemed significantly different
($p < 0.05$) from the others obtained for an individual.}
\label{sfig:univariate}
\end{figure}

Two notable differences between the two perturbation methods are, first, the uniformity in the stability of the
statistics, and second, the dramatic decline in stability of individual statistics in the \new{sparse} perturbation setting
despite the consistency in the overall distribution of values. \new{This result is consistent with that obtained
from the multivariate exploration performed in the body of this article.} It is unclear at present if the discrepancy
between the stability of modularity in the pipeline perturbation context versus the other statistics suggests the
implementation of this measure is the source of instability or if it is implicit to the measure itself. The dramatic
decline in the stability of features derived from \new{sparse} perturbed graphs despite no difference in their overall
distribution both shows that while individual estimates may be unstable the comparison between aggregates or groups may
be considered much more reliable; this finding is consistent with that presented for multivariate statistics.


\end{document}
